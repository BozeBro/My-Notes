- #15418/Lecture2
	- SIMD - Single Instruction, Multiple Data
		- Imagine you have a for loop that computes the same instructions but with different data.
		- broadcast those instructions to multiple ALUs and those ALUs compute same data in parallel.
	- Modern processor optimizations:
		- Use many multiprocessing cores that are simpler than high speed ones. (thread level parallelism > instruction level parallelism)
		- Split instruction execution among many ALUs (SIMD)
			- little extra overhead
		- multi-threading helps to use all processing resources available
		- high arithmetic capability => bandwidth bottleneck in applications (bandwidth?)
		- GPU architectures are same as CPU, but more extreme
	- There is both instruction level parallelism and multi-core parallelism
- Intrinsics of assembly availabe to C language
	- include <immintrin.h>
- #15418/Terms
	- Multi-core processor
	- SIMD execution
	- Coherent Control flow
	- Hardware multi-threading
		- Interleaved multi-threading
		- simultaneous multi-threading
	- memory latency
	- memory bandwidth
	- bandwidth bound application
	- arithmetic intensity
- Read more Lecture 2 of #15418 https://www.cs.cmu.edu/afs/cs/academic/class/15418-s24/public/lectures/02_basicarch.pdf
- #15418 09:42 In lecture now
	- Using GPUs (graphics processing Unit) like a parallelable processor
	- Brook stream programming language that compiles code into openGL type stuff (makes the hack easier to do)
	- NVIDIA Tesla architecture (2007) - "compute mode" interface to GPU hardware.\
		- Run non-graphics program on GPU programmable core
		- CUDE programming language introduced with NVIDIA
- #15418/CUDA Hierarchy of concurrent threads.
	- Create grid of blocks of threads
		- BlockIdx and ThreadIdx
	- __global__ defines kernel function to run on GPU
	- matrixAdd<<<numBlocks, threadsPerBlock>>>(A,B,C)
		- says to run on CUDA with these blocks and threads per block
	- Kernel = core part of our computation (CUDA), not the OS kernel
	- Granularity must be chosen manually (beware of out of bounds for threads)
	- Host (serial execution) and CUDA device (SPMD execution) have different memory spaces and CPU and GPU must share data being sent to each other.
	- ![15418 lecture 3.pdf](../assets/15418_lecture_3_1706108611362_0.pdf) Reference slides
	- __shared__ defines shared memory in a block in CUDA
	- __syncthreads()
		- Barrier: wait for all threads in the block to arrive at this point
	- Atomic operations e.g. float atomicAdd(...)
	- Host/device synchronization
		- implicit barrier across all threads at return of kernel
	- three types of device address spaces (thread, block shared, program shared)
	- NVIDIA GTX 980 (2014)
	- NVIDIA GPUs group 32 CUDA threads into a shared instruction stream called warps.
	- Review of a warp:
		- 32 threads in a block that are running the same instructions
		- Need to do masks when you have threads that are doing different computations
	- Thread blocks can be scheduled in any order (should be order independent of thread blocks)
		- When block is executing, all threads are running
- DONE Do homework for #Writing and print out resume for Friday
  :LOGBOOK:
  CLOCK: [2024-01-27 Sat 23:13:39]--[2024-01-28 Sun 12:53:42] =>  13:40:03
  :END: